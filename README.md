# 11785 Team 20 Project: On Adversarial Robustness and Out-of-Distribution Robustness of Large Language Models

This repository contains the code we developed to evaluate the correlation between the adversarial and out-of-distribution robustness of several LLMs. The experiment details can be found [here](https://drive.google.com/file/d/1BTr7b6THeWSonS3ljpzYTznAFCzEp4gs/view?usp=sharing).


## Project Setup
We conducted the evaluation using Ollama and Nvidia T4 GPUs. Before getting started, you will need access to compute capable of repetitive LLM inference using llama2:7b, llama2:13b, and mixtral:8x7b